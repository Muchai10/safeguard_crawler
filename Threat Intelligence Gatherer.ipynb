{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "V1",
   "id": "2a71df522774418f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-17T17:52:15.964742Z",
     "start_time": "2025-11-17T17:48:02.772370Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# NLTK download\n",
    "# -------------------------------------------------------\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# KEYWORD LISTS\n",
    "# -------------------------------------------------------\n",
    "GBV_KEYWORDS = [\n",
    "    'gender based violence', 'gbv', 'domestic violence',\n",
    "    'sexual harassment', 'women abuse', 'femicide'\n",
    "]\n",
    "CYBERBULLYING_KEYWORDS = [\n",
    "    'cyber bullying', 'online harassment', 'cyberbullying',\n",
    "    'internet trolling', 'social media abuse'\n",
    "]\n",
    "SCAMS_KEYWORDS = [\n",
    "    'scam', 'fraud', 'online scam', 'phishing',\n",
    "    'cyber fraud', 'investment scam'\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CATEGORIZATION\n",
    "# -------------------------------------------------------\n",
    "def categorize_article(text):\n",
    "    if not text:\n",
    "        return \"Other\"\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    scores = {\"GBV\": 0, \"Cyberbullying\": 0, \"Scams\": 0}\n",
    "\n",
    "    for s in sentences:\n",
    "        s = s.lower()\n",
    "        if any(k in s for k in GBV_KEYWORDS): scores[\"GBV\"] += 1\n",
    "        if any(k in s for k in CYBERBULLYING_KEYWORDS): scores[\"Cyberbullying\"] += 1\n",
    "        if any(k in s for k in SCAMS_KEYWORDS): scores[\"Scams\"] += 1\n",
    "\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# LINK EXTRACTION\n",
    "# -------------------------------------------------------\n",
    "def extract_article_links(html, base_url, max_links=5):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = set()\n",
    "\n",
    "    selectors = [\n",
    "        'article a[href]',\n",
    "        'h2 a[href]', 'h3 a[href]',\n",
    "        '.story a[href]', '.news-item a[href]',\n",
    "        'a[href*=\"/news/\"]', 'a[href*=\"/article/\"]', 'a[href*=\"/story/\"]'\n",
    "    ]\n",
    "\n",
    "    for selector in selectors:\n",
    "        for elem in soup.select(selector)[:max_links * 2]:\n",
    "            href = elem.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            full = urljoin(base_url, href)\n",
    "\n",
    "            if urlparse(full).netloc == urlparse(base_url).netloc:\n",
    "                links.add(full)\n",
    "                if len(links) >= max_links:\n",
    "                    return list(links)\n",
    "\n",
    "    return list(links)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# DATE CHECK\n",
    "# -------------------------------------------------------\n",
    "def is_recent(date, days=30):\n",
    "    if not date:\n",
    "        return True\n",
    "    return (datetime.now() - date).days <= days\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# MAIN SCRAPER\n",
    "# -------------------------------------------------------\n",
    "def main():\n",
    "\n",
    "    site_urls = [\n",
    "        'https://www.the-star.co.ke/',\n",
    "        'https://www.tuko.co.ke/'\n",
    "    ]\n",
    "\n",
    "    max_articles = 5\n",
    "    data = []\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Selenium Setup (FIXED)\n",
    "    # ---------------------------------------------------\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Scraping Loop\n",
    "    # ---------------------------------------------------\n",
    "    for site in site_urls:\n",
    "        print(f\"\\nðŸ”µ Loading site: {site}\")\n",
    "\n",
    "        try:\n",
    "            driver.get(site)\n",
    "            time.sleep(3)\n",
    "\n",
    "            html = driver.page_source\n",
    "            article_links = extract_article_links(html, site, max_articles)\n",
    "\n",
    "            print(f\" â†’ Found {len(article_links)} article links\")\n",
    "\n",
    "            for url in article_links:\n",
    "                try:\n",
    "                    print(f\"   ðŸ“° Processing: {url}\")\n",
    "\n",
    "                    article = Article(url)\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "\n",
    "                    if not is_recent(article.publish_date):\n",
    "                        print(\"     â³ Old article â†’ Skipped\")\n",
    "                        continue\n",
    "\n",
    "                    text = article.text\n",
    "                    category = categorize_article(text)\n",
    "\n",
    "                    data.append({\n",
    "                        \"site_url\": site,\n",
    "                        \"article_url\": url,\n",
    "                        \"title\": article.title,\n",
    "                        \"publish_date\": article.publish_date.strftime(\"%Y-%m-%d\") if article.publish_date else \"Unknown\",\n",
    "                        \"category\": category,\n",
    "                        \"summary_snippet\": text[:200] + \"...\" if len(text) > 200 else text\n",
    "                    })\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"     âŒ Article parsing error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Site loading error: {e}\")\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # SAVE RESULTS\n",
    "    # ---------------------------------------------------\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"articles_from_sites.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nâœ… Saved {len(data)} articles to articles_from_sites.csv\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"\\nâš  No articles scraped.\")\n",
    "\n",
    "# Run\n",
    "main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ Loading site: https://www.the-star.co.ke/\n",
      " â†’ Found 5 article links\n",
      "   ðŸ“° Processing: https://www.the-star.co.ke/video/2025-11-17-ps-bitok-kjsea-results-to-be-released-by-december-11\n",
      "   ðŸ“° Processing: https://www.the-star.co.ke/opinion/star-blogs/2025-11-16-odm-must-find-its-ideological-soul-to-survive-beyond-raila\n",
      "   ðŸ“° Processing: https://www.the-star.co.ke/news/infographics/2025-11-17-kenyas-acute-food-insecurity-outlook\n",
      "   ðŸ“° Processing: https://www.the-star.co.ke/business/kenya/2025-11-17-senate-taxpayers-association-to-address-gaps-in-agriculture-governance\n",
      "   ðŸ“° Processing: https://www.the-star.co.ke/video\n",
      "\n",
      "ðŸ”µ Loading site: https://www.tuko.co.ke/\n",
      " â†’ Found 0 article links\n",
      "\n",
      "âœ… Saved 5 articles to articles_from_sites.csv\n",
      "                      site_url  \\\n",
      "0  https://www.the-star.co.ke/   \n",
      "1  https://www.the-star.co.ke/   \n",
      "2  https://www.the-star.co.ke/   \n",
      "3  https://www.the-star.co.ke/   \n",
      "4  https://www.the-star.co.ke/   \n",
      "\n",
      "                                         article_url  \\\n",
      "0  https://www.the-star.co.ke/video/2025-11-17-ps...   \n",
      "1  https://www.the-star.co.ke/opinion/star-blogs/...   \n",
      "2  https://www.the-star.co.ke/news/infographics/2...   \n",
      "3  https://www.the-star.co.ke/business/kenya/2025...   \n",
      "4                   https://www.the-star.co.ke/video   \n",
      "\n",
      "                                               title publish_date category  \\\n",
      "0  PS Bitok KJSEA results to be released by Decem...   2025-11-17    Other   \n",
      "1  ODM must find its ideological soul to survive ...   2025-11-16    Other   \n",
      "2              Kenya's acute food insecurity outlook   2025-11-17    Other   \n",
      "3  Senators, tax experts plan economic forum to r...   2025-11-17    Other   \n",
      "4                                              video      Unknown    Other   \n",
      "\n",
      "                                     summary_snippet  \n",
      "0                                                     \n",
      "1  As Raila Odingaâ€™s body was laid to rest in Bon...  \n",
      "2  Key concerns include below-average rains forec...  \n",
      "3  NTA officials, led by CEO Patrick Nyangweso, w...  \n",
      "4                                                     \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "V 2- Database",
   "id": "501f9e1e6ba561ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:26:49.517271Z",
     "start_time": "2025-11-17T18:25:01.545645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# Enhanced Article Scraper with PostgreSQL, Elasticsearch, and Celery\n",
    "# =====================================================================\n",
    "# This script adapts the original scraper to:\n",
    "# - Use PostgreSQL (via SQLAlchemy) for structured storage.\n",
    "# - Use Elasticsearch for full-text search indexing.\n",
    "# - Use Celery with Redis for asynchronous tasking (scraping sites in background tasks).\n",
    "#\n",
    "# Assumptions:\n",
    "# - PostgreSQL DB: Create a database named 'articles_db' (or update DATABASE_URL). Ensure server is running on localhost:5432.\n",
    "# - Elasticsearch: Running on localhost:9200 (update ES_URL if needed).\n",
    "# - Redis: Running on localhost:6379 (Celery broker).\n",
    "# - In Jupyter Notebook (PyCharm): Run cells sequentially. First setup cell (including NLTK download),\n",
    "#   then start Celery worker (in terminal: celery -A scraper worker --loglevel=info),\n",
    "#   then queue tasks.\n",
    "#\n",
    "# Usage in Jupyter:\n",
    "# 1. Run setup (imports, models, Celery app) - this creates tables if DB connected.\n",
    "# 2. NLTK download happens automatically in setup.\n",
    "# 3. In a separate terminal in PyCharm: Activate env and run `celery -A your_notebook_module worker --loglevel=info` (save as .py or use module name).\n",
    "# 4. Queue tasks: results = main()\n",
    "# 5. Monitor worker logs; query DB/ES as needed.\n",
    "#\n",
    "# Fixed Issues:\n",
    "# - SQLAlchemy 2.0+ deprecation: Updated declarative_base import.\n",
    "# - DB Model: publish_date as String (matches stored format; avoids parsing errors for \"Unknown\").\n",
    "# - ES Search: Fixed bool query structure for category filtering.\n",
    "# - Added NLTK download back.\n",
    "# - Minor: Ensured publish_date handling in task (None for Unknown in model if needed, but String now).\n",
    "# - Tested non-external parts (e.g., categorization logic) mentally; full run requires local services.\n",
    "# =====================================================================\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# IMPORTS\n",
    "# -------------------------------------------------------\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# New imports for DB, ES, Celery\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker  # Fixed: Use orm.declarative_base for SQLAlchemy 2.0+\n",
    "from elasticsearch import Elasticsearch\n",
    "from celery import Celery\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# NLTK DOWNLOAD (added back)\n",
    "# -------------------------------------------------------\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------------------------------\n",
    "# Database (PostgreSQL) - UPDATE WITH YOUR ACTUAL CREDS!\n",
    "DATABASE_URL = \"postgresql://postgres:your_password@localhost/articles_db\"  # e.g., postgres:pass123@localhost/articles_db\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# Elasticsearch\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "es_client = Elasticsearch([ES_URL])\n",
    "\n",
    "# Celery (Redis broker)\n",
    "celery_app = Celery('scraper', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')\n",
    "celery_app.conf.update(\n",
    "    task_serializer='json',\n",
    "    accept_content=['json'],\n",
    "    result_serializer='json',\n",
    "    timezone='Africa/Nairobi',\n",
    "    enable_utc=True,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# DATABASE MODEL (publish_date as String to match stored format)\n",
    "# -------------------------------------------------------\n",
    "class Article(Base):\n",
    "    __tablename__ = \"articles\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    site_url = Column(String, index=True)\n",
    "    article_url = Column(String, unique=True, index=True)\n",
    "    title = Column(String)\n",
    "    publish_date = Column(String(50))  # Changed to String for \"%Y-%m-%d\" or \"Unknown\"\n",
    "    category = Column(String)\n",
    "    summary_snippet = Column(Text)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "# Create tables (run once) - Will fail if DB not connected/running!\n",
    "Base.metadata.create_all(bind=engine)\n",
    "print(\"âœ… Tables created (or already exist).\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# KEYWORD LISTS (unchanged)\n",
    "# -------------------------------------------------------\n",
    "GBV_KEYWORDS = [\n",
    "    'gender based violence', 'gbv', 'domestic violence', 'sexual harassment', 'women abuse', 'femicide'\n",
    "]\n",
    "CYBERBULLYING_KEYWORDS = [\n",
    "    'cyber bullying', 'online harassment', 'cyberbullying', 'internet trolling', 'social media abuse'\n",
    "]\n",
    "SCAMS_KEYWORDS = [\n",
    "    'scam', 'fraud', 'online scam', 'phishing', 'cyber fraud', 'investment scam'\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CATEGORIZATION (unchanged)\n",
    "# -------------------------------------------------------\n",
    "def categorize_article(text):\n",
    "    if not text:\n",
    "        return \"Other\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    scores = {\"GBV\": 0, \"Cyberbullying\": 0, \"Scams\": 0}\n",
    "    for s in sentences:\n",
    "        s = s.lower()\n",
    "        if any(k in s for k in GBV_KEYWORDS):\n",
    "            scores[\"GBV\"] += 1\n",
    "        if any(k in s for k in CYBERBULLYING_KEYWORDS):\n",
    "            scores[\"Cyberbullying\"] += 1\n",
    "        if any(k in s for k in SCAMS_KEYWORDS):\n",
    "            scores[\"Scams\"] += 1\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# LINK EXTRACTION (unchanged)\n",
    "# -------------------------------------------------------\n",
    "def extract_article_links(html, base_url, max_links=5):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = set()\n",
    "    selectors = [\n",
    "        'article a[href]', 'h2 a[href]', 'h3 a[href]', '.story a[href]', '.news-item a[href]',\n",
    "        'a[href*=\"/news/\"]', 'a[href*=\"/article/\"]', 'a[href*=\"/story/\"]'\n",
    "    ]\n",
    "    for selector in selectors:\n",
    "        for elem in soup.select(selector)[:max_links * 2]:\n",
    "            href = elem.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            full = urljoin(base_url, href)\n",
    "            if urlparse(full).netloc == urlparse(base_url).netloc:\n",
    "                links.add(full)\n",
    "            if len(links) >= max_links:\n",
    "                return list(links)\n",
    "    return list(links)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# DATE CHECK (unchanged)\n",
    "# -------------------------------------------------------\n",
    "def is_recent(date, days=30):\n",
    "    if not date:\n",
    "        return True\n",
    "    return (datetime.now() - date).days <= days\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SAVE TO POSTGRESQL\n",
    "# -------------------------------------------------------\n",
    "def save_to_db(article_data):\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        # Ensure publish_date is str\n",
    "        if article_data[\"publish_date\"] == \"Unknown\":\n",
    "            article_data[\"publish_date\"] = None  # Optional: Set to None for cleaner data\n",
    "        db_article = Article(**article_data)\n",
    "        db.add(db_article)\n",
    "        db.commit()\n",
    "        db.refresh(db_article)\n",
    "        return db_article.id\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        print(f\"âŒ DB Save error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# INDEX TO ELASTICSEARCH\n",
    "# -------------------------------------------------------\n",
    "def index_to_es(article_data, db_id):\n",
    "    doc = {\n",
    "        \"id\": db_id,\n",
    "        \"site_url\": article_data[\"site_url\"],\n",
    "        \"article_url\": article_data[\"article_url\"],\n",
    "        \"title\": article_data[\"title\"],\n",
    "        \"publish_date\": article_data[\"publish_date\"],\n",
    "        \"category\": article_data[\"category\"],\n",
    "        \"summary_snippet\": article_data[\"summary_snippet\"],\n",
    "        \"created_at\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "    try:\n",
    "        es_client.index(index=\"articles\", id=db_id, body=doc)\n",
    "        print(f\"âœ… Indexed to ES: {article_data['title'][:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ES Index error: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CELERY TASK: SCRAPE SINGLE SITE\n",
    "# -------------------------------------------------------\n",
    "@celery_app.task(bind=True)\n",
    "def scrape_site_task(self, site_url, max_articles=5):\n",
    "    print(f\"ðŸ”µ Task started: Scraping {site_url}\")\n",
    "\n",
    "    # Selenium Setup\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        driver.get(site_url)\n",
    "        time.sleep(3)\n",
    "        html = driver.page_source\n",
    "        article_links = extract_article_links(html, site_url, max_articles)\n",
    "        print(f\" â†’ Found {len(article_links)} article links\")\n",
    "\n",
    "        processed_count = 0\n",
    "        for url in article_links:\n",
    "            try:\n",
    "                print(f\" ðŸ“° Processing: {url}\")\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "\n",
    "                if not is_recent(article.publish_date):\n",
    "                    print(\" â³ Old article â†’ Skipped\")\n",
    "                    continue\n",
    "\n",
    "                text = article.text\n",
    "                category = categorize_article(text)\n",
    "\n",
    "                publish_date_str = article.publish_date.strftime(\"%Y-%m-%d\") if article.publish_date else \"Unknown\"\n",
    "                article_data = {\n",
    "                    \"site_url\": site_url,\n",
    "                    \"article_url\": url,\n",
    "                    \"title\": article.title,\n",
    "                    \"publish_date\": publish_date_str,\n",
    "                    \"category\": category,\n",
    "                    \"summary_snippet\": text[:200] + \"...\" if len(text) > 200 else text\n",
    "                }\n",
    "\n",
    "                # Save to PG\n",
    "                db_id = save_to_db(article_data)\n",
    "                if db_id:\n",
    "                    # Index to ES\n",
    "                    index_to_es(article_data, db_id)\n",
    "                    processed_count += 1\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" âŒ Article parsing error: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"âœ… Task completed: Processed {processed_count} articles from {site_url}\")\n",
    "        return {\"site\": site_url, \"processed\": processed_count}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Site loading error: {e}\")\n",
    "        raise self.retry(countdown=60)  # Retry after 1 min on failure\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# MAIN (for queuing tasks - run in Jupyter)\n",
    "# -------------------------------------------------------\n",
    "def main():\n",
    "    site_urls = [\n",
    "        'https://www.the-star.co.ke/',\n",
    "        'https://www.tuko.co.ke/'\n",
    "    ]\n",
    "    max_articles = 5\n",
    "\n",
    "    # Queue tasks asynchronously\n",
    "    results = []\n",
    "    for site in site_urls:\n",
    "        result = scrape_site_task.delay(site, max_articles)\n",
    "        results.append(result)\n",
    "\n",
    "    print(f\"ðŸš€ Queued {len(results)} tasks. Check Celery worker logs for progress.\")\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# QUERY EXAMPLES (for Jupyter) - Fixed ES search structure\n",
    "# -------------------------------------------------------\n",
    "def query_db(category=None):\n",
    "    \"\"\"Query articles from PostgreSQL.\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        query = db.query(Article)\n",
    "        if category:\n",
    "            query = query.filter(Article.category == category)\n",
    "        df = pd.read_sql(query.statement, engine)\n",
    "        return df\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def search_es(query_str, category=None):\n",
    "    \"\"\"Search articles in Elasticsearch.\"\"\"\n",
    "    multi_match = {\"multi_match\": {\"query\": query_str, \"fields\": [\"title\", \"summary_snippet\"]}}\n",
    "\n",
    "    if category:\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [multi_match],\n",
    "                    \"filter\": [{\"term\": {\"category\": category}}]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        search_body = {\"query\": multi_match}\n",
    "\n",
    "    try:\n",
    "        res = es_client.search(index=\"articles\", body=search_body)\n",
    "        return [hit[\"_source\"] for hit in res[\"hits\"][\"hits\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ES Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage in Jupyter:\n",
    "# results = main()  # Queues tasks\n",
    "# df = query_db(\"GBV\")  # After tasks complete\n",
    "# es_results = search_es(\"scam\", \"Scams\")\n",
    "# print(df.head() if not df.empty else \"No data yet.\")"
   ],
   "id": "6920ba7d97972dda",
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOperationalError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     \u001B[38;5;28mself\u001B[39m._dbapi_connection = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraw_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3301\u001B[39m, in \u001B[36mEngine.raw_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3280\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001B[39;00m\n\u001B[32m   3281\u001B[39m \n\u001B[32m   3282\u001B[39m \u001B[33;03mThe returned object is a proxied version of the DBAPI\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3299\u001B[39m \n\u001B[32m   3300\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3301\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001B[39m, in \u001B[36mPool.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    440\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a DBAPI connection from the pool.\u001B[39;00m\n\u001B[32m    441\u001B[39m \n\u001B[32m    442\u001B[39m \u001B[33;03mThe connection is instrumented such that when its\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    445\u001B[39m \n\u001B[32m    446\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionFairy\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_checkout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001B[39m, in \u001B[36m_ConnectionFairy._checkout\u001B[39m\u001B[34m(cls, pool, threadconns, fairy)\u001B[39m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fairy:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m     fairy = \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcheckout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1266\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m threadconns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001B[39m, in \u001B[36m_ConnectionRecord.checkout\u001B[39m\u001B[34m(cls, pool)\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m711\u001B[39m     rec = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_do_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dec_overflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001B[39m, in \u001B[36mPool._create_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    386\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001B[39m, in \u001B[36m_ConnectionRecord.__init__\u001B[39m\u001B[34m(self, pool, connect)\u001B[39m\n\u001B[32m    672\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connect:\n\u001B[32m--> \u001B[39m\u001B[32m673\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[38;5;28mself\u001B[39m.finalize_callback = deque()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    898\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m899\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mError on connect(): \u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    894\u001B[39m \u001B[38;5;28mself\u001B[39m.starttime = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m895\u001B[39m \u001B[38;5;28mself\u001B[39m.dbapi_connection = connection = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_invoke_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    896\u001B[39m pool.logger.debug(\u001B[33m\"\u001B[39m\u001B[33mCreated new connection \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m, connection)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001B[39m, in \u001B[36mcreate_engine.<locals>.connect\u001B[39m\u001B[34m(connection_record)\u001B[39m\n\u001B[32m    659\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m connection\n\u001B[32m--> \u001B[39m\u001B[32m661\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdialect\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:629\u001B[39m, in \u001B[36mDefaultDialect.connect\u001B[39m\u001B[34m(self, *cargs, **cparams)\u001B[39m\n\u001B[32m    627\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001B[32m    628\u001B[39m     \u001B[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloaded_dbapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001B[39m, in \u001B[36mconnect\u001B[39m\u001B[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001B[39m\n\u001B[32m    134\u001B[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m conn = \u001B[43m_connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconnection_factory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconnection_factory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwasync\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cursor_factory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mOperationalError\u001B[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mOperationalError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 102\u001B[39m\n\u001B[32m     99\u001B[39m     created_at = Column(DateTime, default=datetime.utcnow)\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# Create tables (run once) - Will fail if DB not connected/running!\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m \u001B[43mBase\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbind\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mâœ… Tables created (or already exist).\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    105\u001B[39m \u001B[38;5;66;03m# -------------------------------------------------------\u001B[39;00m\n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# KEYWORD LISTS (unchanged)\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;66;03m# -------------------------------------------------------\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:5924\u001B[39m, in \u001B[36mMetaData.create_all\u001B[39m\u001B[34m(self, bind, tables, checkfirst)\u001B[39m\n\u001B[32m   5900\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_all\u001B[39m(\n\u001B[32m   5901\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   5902\u001B[39m     bind: _CreateDropBind,\n\u001B[32m   5903\u001B[39m     tables: Optional[_typing_Sequence[Table]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   5904\u001B[39m     checkfirst: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   5905\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5906\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create all tables stored in this metadata.\u001B[39;00m\n\u001B[32m   5907\u001B[39m \n\u001B[32m   5908\u001B[39m \u001B[33;03m    Conditional by default, will not attempt to recreate tables already\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   5922\u001B[39m \n\u001B[32m   5923\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m5924\u001B[39m     \u001B[43mbind\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_run_ddl_visitor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5925\u001B[39m \u001B[43m        \u001B[49m\u001B[43mddl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSchemaGenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckfirst\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheckfirst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtables\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtables\u001B[49m\n\u001B[32m   5926\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3251\u001B[39m, in \u001B[36mEngine._run_ddl_visitor\u001B[39m\u001B[34m(self, visitorcallable, element, **kwargs)\u001B[39m\n\u001B[32m   3245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_ddl_visitor\u001B[39m(\n\u001B[32m   3246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   3247\u001B[39m     visitorcallable: Type[InvokeDDLBase],\n\u001B[32m   3248\u001B[39m     element: SchemaVisitable,\n\u001B[32m   3249\u001B[39m     **kwargs: Any,\n\u001B[32m   3250\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3251\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_run_ddl_visitor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvisitorcallable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43melement\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:141\u001B[39m, in \u001B[36m_GeneratorContextManager.__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args, \u001B[38;5;28mself\u001B[39m.kwds, \u001B[38;5;28mself\u001B[39m.func\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    143\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mgenerator didn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt yield\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3241\u001B[39m, in \u001B[36mEngine.begin\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3216\u001B[39m \u001B[38;5;129m@contextlib\u001B[39m.contextmanager\n\u001B[32m   3217\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbegin\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Iterator[Connection]:\n\u001B[32m   3218\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a context manager delivering a :class:`_engine.Connection`\u001B[39;00m\n\u001B[32m   3219\u001B[39m \u001B[33;03m    with a :class:`.Transaction` established.\u001B[39;00m\n\u001B[32m   3220\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   3239\u001B[39m \n\u001B[32m   3240\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3241\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m conn:\n\u001B[32m   3242\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m conn.begin():\n\u001B[32m   3243\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m conn\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3277\u001B[39m, in \u001B[36mEngine.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3254\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Connection:\n\u001B[32m   3255\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001B[39;00m\n\u001B[32m   3256\u001B[39m \n\u001B[32m   3257\u001B[39m \u001B[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3274\u001B[39m \n\u001B[32m   3275\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    143\u001B[39m         \u001B[38;5;28mself\u001B[39m._dbapi_connection = engine.raw_connection()\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m         \u001B[43mConnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle_dbapi_exception_noconnection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[43m            \u001B[49m\u001B[43merr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdialect\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2440\u001B[39m, in \u001B[36mConnection._handle_dbapi_exception_noconnection\u001B[39m\u001B[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001B[39m\n\u001B[32m   2438\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m should_wrap:\n\u001B[32m   2439\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m sqlalchemy_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2440\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001B[32m2\u001B[39m]) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   2441\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2442\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_info[\u001B[32m1\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m         \u001B[38;5;28mself\u001B[39m._dbapi_connection = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraw_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    145\u001B[39m         Connection._handle_dbapi_exception_noconnection(\n\u001B[32m    146\u001B[39m             err, dialect, engine\n\u001B[32m    147\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3301\u001B[39m, in \u001B[36mEngine.raw_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3279\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mraw_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> PoolProxiedConnection:\n\u001B[32m   3280\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001B[39;00m\n\u001B[32m   3281\u001B[39m \n\u001B[32m   3282\u001B[39m \u001B[33;03m    The returned object is a proxied version of the DBAPI\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3299\u001B[39m \n\u001B[32m   3300\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3301\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001B[39m, in \u001B[36mPool.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    439\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> PoolProxiedConnection:\n\u001B[32m    440\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a DBAPI connection from the pool.\u001B[39;00m\n\u001B[32m    441\u001B[39m \n\u001B[32m    442\u001B[39m \u001B[33;03m    The connection is instrumented such that when its\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    445\u001B[39m \n\u001B[32m    446\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionFairy\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_checkout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001B[39m, in \u001B[36m_ConnectionFairy._checkout\u001B[39m\u001B[34m(cls, pool, threadconns, fairy)\u001B[39m\n\u001B[32m   1256\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m   1257\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_checkout\u001B[39m(\n\u001B[32m   1258\u001B[39m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1261\u001B[39m     fairy: Optional[_ConnectionFairy] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1262\u001B[39m ) -> _ConnectionFairy:\n\u001B[32m   1263\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fairy:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m         fairy = \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcheckout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1266\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m threadconns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1267\u001B[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001B[39m, in \u001B[36m_ConnectionRecord.checkout\u001B[39m\u001B[34m(cls, pool)\u001B[39m\n\u001B[32m    709\u001B[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m711\u001B[39m     rec = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_do_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    714\u001B[39m     dbapi_connection = rec.get_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_connection()\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dec_overflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    222\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    226\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    173\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._inc_overflow():\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m175\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    176\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m    177\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m util.safe_reraise():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001B[39m, in \u001B[36mPool._create_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_create_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> ConnectionPoolEntry:\n\u001B[32m    386\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001B[39m, in \u001B[36m_ConnectionRecord.__init__\u001B[39m\u001B[34m(self, pool, connect)\u001B[39m\n\u001B[32m    671\u001B[39m \u001B[38;5;28mself\u001B[39m.__pool = pool\n\u001B[32m    672\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connect:\n\u001B[32m--> \u001B[39m\u001B[32m673\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[38;5;28mself\u001B[39m.finalize_callback = deque()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    897\u001B[39m     \u001B[38;5;28mself\u001B[39m.fresh = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    898\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m899\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mError on connect(): \u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    902\u001B[39m     \u001B[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001B[39;00m\n\u001B[32m    903\u001B[39m     \u001B[38;5;66;03m# the engine, so this will usually not be set\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    222\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    226\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    893\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    894\u001B[39m     \u001B[38;5;28mself\u001B[39m.starttime = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m895\u001B[39m     \u001B[38;5;28mself\u001B[39m.dbapi_connection = connection = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_invoke_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    896\u001B[39m     pool.logger.debug(\u001B[33m\"\u001B[39m\u001B[33mCreated new connection \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m, connection)\n\u001B[32m    897\u001B[39m     \u001B[38;5;28mself\u001B[39m.fresh = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001B[39m, in \u001B[36mcreate_engine.<locals>.connect\u001B[39m\u001B[34m(connection_record)\u001B[39m\n\u001B[32m    658\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    659\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m connection\n\u001B[32m--> \u001B[39m\u001B[32m661\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdialect\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:629\u001B[39m, in \u001B[36mDefaultDialect.connect\u001B[39m\u001B[34m(self, *cargs, **cparams)\u001B[39m\n\u001B[32m    627\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001B[32m    628\u001B[39m     \u001B[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloaded_dbapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001B[39m, in \u001B[36mconnect\u001B[39m\u001B[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001B[39m\n\u001B[32m    132\u001B[39m     kwasync[\u001B[33m'\u001B[39m\u001B[33masync_\u001B[39m\u001B[33m'\u001B[39m] = kwargs.pop(\u001B[33m'\u001B[39m\u001B[33masync_\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    134\u001B[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m conn = \u001B[43m_connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconnection_factory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconnection_factory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwasync\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cursor_factory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    137\u001B[39m     conn.cursor_factory = cursor_factory\n",
      "\u001B[31mOperationalError\u001B[39m: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "V3",
   "id": "2b726e962047e06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# NLTK download\n",
    "# -------------------------------------------------------\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# KEYWORD LISTS (original for categorization)\n",
    "# -------------------------------------------------------\n",
    "GBV_KEYWORDS = [\n",
    "    'gender based violence', 'gbv', 'domestic violence', 'sexual harassment', 'women abuse', 'femicide'\n",
    "]\n",
    "CYBERBULLYING_KEYWORDS = [\n",
    "    'cyber bullying', 'online harassment', 'cyberbullying', 'internet trolling', 'social media abuse'\n",
    "]\n",
    "SCAMS_KEYWORDS = [\n",
    "    'scam', 'fraud', 'online scam', 'phishing', 'cyber fraud', 'investment scam'\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# CATEGORIZATION (original keyword-based)\n",
    "# -------------------------------------------------------\n",
    "def categorize_article(text):\n",
    "    if not text:\n",
    "        return \"Other\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    scores = {\"GBV\": 0, \"Cyberbullying\": 0, \"Scams\": 0}\n",
    "    for s in sentences:\n",
    "        s = s.lower()\n",
    "        if any(k in s for k in GBV_KEYWORDS):\n",
    "            scores[\"GBV\"] += 1\n",
    "        if any(k in s for k in CYBERBULLYING_KEYWORDS):\n",
    "            scores[\"Cyberbullying\"] += 1\n",
    "        if any(k in s for k in SCAMS_KEYWORDS):\n",
    "            scores[\"Scams\"] += 1\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# LINK EXTRACTION (original, cached)\n",
    "# -------------------------------------------------------\n",
    "@lru_cache(maxsize=10)\n",
    "def extract_article_links(html, base_url, max_links=5):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = set()\n",
    "    selectors = [\n",
    "        'article a[href]', 'h2 a[href]', 'h3 a[href]', '.story a[href]', '.news-item a[href]',\n",
    "        'a[href*=\"/news/\"]', 'a[href*=\"/article/\"]', 'a[href*=\"/story/\"]'\n",
    "    ]\n",
    "    for selector in selectors:\n",
    "        for elem in soup.select(selector)[:max_links * 2]:\n",
    "            href = elem.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            full = urljoin(base_url, href)\n",
    "            if urlparse(full).netloc == urlparse(base_url).netloc:\n",
    "                links.add(full)\n",
    "            if len(links) >= max_links:\n",
    "                return list(links)\n",
    "    return list(links)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# DATE CHECK (original)\n",
    "# -------------------------------------------------------\n",
    "def is_recent(date, days=30):\n",
    "    if not date:\n",
    "        return True\n",
    "    return (datetime.now() - date).days <= days\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# ARTICLE PROCESSOR (for parallel execution)\n",
    "# -------------------------------------------------------\n",
    "def process_article(url, site_url, sentiment_pipeline, topic_pipeline, ner_pipeline):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if not is_recent(article.publish_date):\n",
    "            return None\n",
    "        text = article.text\n",
    "        if not text or len(text.split()) < 50:  # Skip very short texts for speed\n",
    "            return None\n",
    "\n",
    "        # Original keyword categorization\n",
    "        category = categorize_article(text)\n",
    "\n",
    "        # Truncate for models\n",
    "        truncated_text = ' '.join(text.split()[:512])  # Token-aware truncate\n",
    "\n",
    "        # Sentiment\n",
    "        sentiment = \"N/A\"\n",
    "        if sentiment_pipeline:\n",
    "            sentiment_result = sentiment_pipeline(truncated_text)\n",
    "            sentiment = sentiment_result[0]['label']\n",
    "\n",
    "        # Topic (zero-shot, lighter model)\n",
    "        topic = \"N/A\"\n",
    "        if topic_pipeline:\n",
    "            candidate_labels = [\n",
    "                \"hate speech\", \"scam\", \"gender based violence\", \"cyber bullying\",\n",
    "                \"high risk crime location\", \"neutral\"\n",
    "            ]\n",
    "            topic_result = topic_pipeline(truncated_text, candidate_labels=candidate_labels)\n",
    "            topic = topic_result['labels'][0]\n",
    "\n",
    "        # NER (locations/orgs/misc)\n",
    "        ner_entities = \"[]\"\n",
    "        if ner_pipeline:\n",
    "            ner_result = ner_pipeline(truncated_text)\n",
    "            relevant_entities = [ent['word'] for ent in ner_result if ent['entity_group'] in ['LOC', 'ORG', 'MISC']]\n",
    "            ner_entities = str(relevant_entities)\n",
    "\n",
    "        return {\n",
    "            \"site_url\": site_url,\n",
    "            \"article_url\": url,\n",
    "            \"title\": article.title,\n",
    "            \"publish_date\": article.publish_date.strftime(\"%Y-%m-%d\") if article.publish_date else \"Unknown\",\n",
    "            \"category\": category,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"topic\": topic,\n",
    "            \"ner_entities\": ner_entities,\n",
    "            \"summary_snippet\": text[:200] + \"...\" if len(text) > 200 else text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\" âŒ Article processing error for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# MAIN SCRAPER\n",
    "# -------------------------------------------------------\n",
    "def main():\n",
    "    site_urls = [\n",
    "        'https://www.standardmedia.co.ke/',\n",
    "        'https://www.the-star.co.ke/',\n",
    "        'https://www.tuko.co.ke/'\n",
    "    ]\n",
    "    max_articles = 5\n",
    "    data = []\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Model Pipelines (loaded once for efficiency)\n",
    "    # ---------------------------------------------------\n",
    "    print(\"ðŸ”„ Loading ML models... This may take a few minutes.\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"ðŸ“± Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "    sentiment_pipeline = None\n",
    "    try:\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=device\n",
    "        )\n",
    "        print(\"âœ… Sentiment model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading sentiment model: {e}\")\n",
    "\n",
    "    ner_pipeline = None\n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        print(\"âœ… NER model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading NER model: {e}\")\n",
    "\n",
    "    topic_pipeline = None\n",
    "    try:\n",
    "        # Lighter zero-shot model for speed\n",
    "        topic_pipeline = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n",
    "            device=device\n",
    "        )\n",
    "        print(\"âœ… Topic model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading topic model: {e}\")\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Selenium Setup (with implicit wait for speed)\n",
    "    # ---------------------------------------------------\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")  # For stability\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "    driver.implicitly_wait(5)  # Faster dynamic waits\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Scraping Loop (with parallel article processing)\n",
    "    # ---------------------------------------------------\n",
    "    for site in site_urls:\n",
    "        print(f\"\\nðŸ”µ Loading site: {site}\")\n",
    "        try:\n",
    "            driver.get(site)\n",
    "            time.sleep(2)  # Reduced wait\n",
    "            html = driver.page_source\n",
    "            article_links = extract_article_links(html, site, max_articles)\n",
    "            print(f\" â†’ Found {len(article_links)} article links\")\n",
    "\n",
    "            # Parallel process articles\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:  # Limit threads for stability\n",
    "                futures = [\n",
    "                    executor.submit(process_article, url, site, sentiment_pipeline, topic_pipeline, ner_pipeline)\n",
    "                    for url in article_links\n",
    "                ]\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        data.append(result)\n",
    "                        category = result['category']\n",
    "                        sentiment = result['sentiment']\n",
    "                        topic = result['topic']\n",
    "                        ner_entities = result['ner_entities'][:50] + \"...\" if len(result['ner_entities']) > 50 else result['ner_entities']\n",
    "                        print(f\" ðŸ“Š Category: {category} | Sentiment: {sentiment} | Topic: {topic} | NER: {ner_entities}\")\n",
    "\n",
    "            time.sleep(0.5)  # Brief pause between sites\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Site loading error: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # SAVE RESULTS\n",
    "    # ---------------------------------------------------\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"articles_from_sites.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nâœ… Saved {len(data)} articles to articles_from_sites.csv\")\n",
    "        print(\"\\nðŸ“‹ CSV Columns: site_url, article_url, title, publish_date, category, sentiment, topic, ner_entities, summary_snippet\")\n",
    "        print(df[['title', 'category', 'sentiment', 'topic', 'ner_entities']].head())\n",
    "    else:\n",
    "        print(\"\\nâš  No articles scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bd8b0e173ba45657"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
